FN <- sum(pre[1:m] == 1)
FP <- sum(pre[(m+1):(m+n)] == 0)
TN <- sum(pre[(m+1):(m+n)] == 1)
sensitivity[i] <- TP/(TP + FN)
specificity[i] <- TN/(TN + FP)
}
s <- sort(prob, index.return = TRUE)
sensitivity <- NULL
specificity <- NULL
for (i in 1:(m+n)){
cutoff <- s$x[i]
vector <- NULL
for (j in 1:(m+n)){
if (prob[j] >= cutoff){
vector[j] <- 0
}else{
vector[j] <- 1
}
}
TP <- sum(vector[1:m] == 0)
FN <- sum(vector[1:m] == 1)
FP <- sum(vector[(m+1):(m+n)] == 0)
TN <- sum(vector[(m+1):(m+n)] == 1)
sensitivity[i] <- TP/(TP + FN)
specificity[i] <- TN/(TN + FP)
}
plot(1-specificity, sensitivity, col = 3, type = 'l', lwd = 2)
library(pROC)
m <- 11
n <- 20
prob  <- runif(m + n)
label <- c(rep(0, m), rep(1, n))
roc(label, prob, plot = TRUE, print.thres = TRUE, print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, col = 2, lwd = 3)
s <- sort(prob, index.return = TRUE)
sensitivity <- NULL
specificity <- NULL
for (i in 1:(m+n)){
cutoff <- s$x[i]
vector <- NULL
for (j in 1:(m+n)){
if (prob[j] >= cutoff){
vector[j] <- 0
}else{
vector[j] <- 1
}
}
TP <- sum(vector[1:m] == 0)
FN <- sum(vector[1:m] == 1)
FP <- sum(vector[(m+1):(m+n)] == 0)
TN <- sum(vector[(m+1):(m+n)] == 1)
sensitivity[i] <- TP/(TP + FN)
specificity[i] <- TN/(TN + FP)
}
plot(1-specificity, sensitivity, col = 3, type = 'l', lwd = 2)
1-specificity
a <- roc(label, prob, plot = TRUE, print.thres = TRUE, print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, col = 2, lwd = 3)
plot(1-specificity, sensitivity, col = 3, type = 'l', lwd = 2)
names(a)
a$sensitivities
sensitivity
a$specificities
specificity
library(pROC)
m <- 11
n <- 20
prob  <- runif(m + n)
label <- c(rep(0, m), rep(1, n))
a <- roc(label, prob, plot = TRUE, print.thres = TRUE, print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, col = 2, lwd = 3)
s <- sort(prob, index.return = TRUE)
sensitivity <- NULL
specificity <- NULL
for (i in 2:(m+n)){
cutoff <- s$x[i]
vector <- NULL
for (j in 1:(m+n)){
if (prob[j] >= cutoff){
vector[j] <- 0
}else{
vector[j] <- 1
}
}
TP <- sum(vector[1:m] == 0)
FN <- sum(vector[1:m] == 1)
FP <- sum(vector[(m+1):(m+n)] == 0)
TN <- sum(vector[(m+1):(m+n)] == 1)
sensitivity[i] <- TP/(TP + FN)
specificity[i] <- TN/(TN + FP)
}
plot(1-specificity, sensitivity, col = 3, type = 'l', lwd = 2)
sensitivity
specificity
a$sensitivities
a$specificities
m+n
library(pROC)
m <- 11
n <- 20
prob  <- runif(m + n)
label <- c(rep(0, m), rep(1, n))
a <- roc(label, prob, plot = TRUE, print.thres = TRUE, print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, col = 2, lwd = 3)
s <- sort(prob, index.return = TRUE)
sensitivity <- NULL
specificity <- NULL
for (i in 1:(m+n)){
cutoff <- s$x[i]
vector <- NULL
for (j in 1:(m+n)){
if (prob[j] > cutoff){
vector[j] <- 0
}else{
vector[j] <- 1
}
}
TP <- sum(vector[1:m] == 0)
FN <- sum(vector[1:m] == 1)
FP <- sum(vector[(m+1):(m+n)] == 0)
TN <- sum(vector[(m+1):(m+n)] == 1)
sensitivity[i] <- TP/(TP + FN)
specificity[i] <- TN/(TN + FP)
}
plot(1-specificity, sensitivity, col = 3, type = 'l', lwd = 2)
a$sensitivities
sensitivity
a$specificities
specificity
?pam
library(cluster)
?pam
?dist
x <- matrix(rnorm(100), nrow = 5)
dist(x)
m <- 20
n <- 200
dataset <- matrix(m*n, m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Gene', 1:n, sep = '')))
distant <- dist(dataset, method = "euclidean")
distant
m <- 5
n <- 200
dataset <- matrix(m*n, m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Gene', 1:n, sep = '')))
distant <- dist(dataset, method = "euclidean")
distant
m <- 5
n <- 200
dataset <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Gene', 1:n, sep = '')))
distant <- dist(dataset, method = "euclidean")
distant
m <- 10
n <- 200
dataset <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Gene', 1:n, sep = '')))
distant <- dist(dataset, method = "euclidean")
pam_analysis <- pam(distant,k=2,diss=TRUE)				# Cluster method1: PAM clustering.
PAM_info <- pam_analysis$silinfo$widths
PAM_info
pam_analysis
hic_info <- hclust(distant, method = "complete")		    # Cluster method2: Hierarchical clustering.
hic_info
plclust(hic_info, labels = FALSE, hang = -1)
re <- rect.hclust(hic_info, k = 2)
hic_id <- cutree(hic_info, 2)
hic_id
kmeans <- kmeans(dataset, centers = 2)					    # Cluster method3: K-means clustering.
kmeans
kms_id <- kmeans$cluster
kms_id
cluster <- matrix(0, m, 3)
dimnames(cluster) <- list(rownames(dataset), c("PAM","Hierarchical","K-means"))
for (i in 1:m){
id <- rownames(dataset)[i]
cluster[i,1] <- PAM_info[id,1]
cluster[i,2] <- hic_id[id]
cluster[i,3] <- kms_id[id]
}
cluster
library(mclust)
ARI <- matrix(0, 3, 3, dimnames = list(colnames(cluster), colnames(cluster)))
library(mclust)
ARI <- matrix(0, 3, 3, dimnames = list(colnames(cluster), colnames(cluster)))
for (i in 1:3){
for (j in 1:3){
ARI[i,j] <- adjustedRandIndex(cluster[,i],cluster[,j])
}
}
ARI
rm(list=ls(all=TRUE))
library(randomForest)
rfNews()
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
x <- matrix(runif(m^2), m,5)						# Training dataset : 100 samples with 5 features.
dim(x)
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
y
table(y)
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
n <- 5
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
n <- 100                                            # Feature number
x <- matrix(runif(m*n), m, n)						# Training dataset : 100 samples with 5 features.
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n)						# Training dataset : 100 samples with 5 features.
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
y
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n)						# Training dataset : 100 samples with 5 features.
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
model  <- randomForest(x, y,ntree = 1000, replace = TRUE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
model
names(model)
?randomForest
dim(model$inbag)
model$inbag[,1:10]
max(model$inbag)
k=1
index  <- list()
print(k)
index[[k]]  <- which(model$inbag[k,] == 0)
index
predict_res <- predict(model, x[k,], type = "response", predict.all = TRUE)
predict_res
x[k,][1:10]
dim(x)
?predict
predict(model, x[k,])
?randomForest
rm(list=ls(all=TRUE))
library(randomForest)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n)						# Training dataset : 100 samples with 5 features.
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
model  <- randomForest(x, y,ntree = 1000, replace = TRUE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
result <- matrix(0, m, m)
index  <- list()
k=1
print(k)
index[[k]]  <- which(model$inbag[k,] == 0)
predict_res <- predict(model, x[k,], type = "response", predict.all = TRUE)
i=5
no_i <- which(model$inbag[i,index[[k]]] == 0)
no_i
no_i <- which(model$inbag[i,index[[k]]] == 0)
is_i <- which(model$inbag[i,index[[k]]] == 1)
is_i
predict_res$individual[,no_i]
y[k]
sum(predict_res$individual[,no_i] == y[k])/length(no_i)
sum(predict_res$individual[,is_i] == y[k])/length(is_i)
rm(list=ls(all=TRUE))
library(randomForest)
library(pheatmap)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n)						# Training dataset : 100 samples with 5 features.
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = FALSE, cex = 1)
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = TRUE, cluster_cols = TRUE, display_numbers = FALSE, cex = 1)
rm(list=ls(all=TRUE))
library(randomForest)
library(pheatmap)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Feature', 1:n, sep = '')))
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = FALSE, cluster_cols = FALSE, cex = 1)
model  <- randomForest(x, y,ntree = 1000, replace = TRUE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
result <- matrix(0, m, m)
index  <- list()
for (k in 1:50){
print(k)
index[[k]]  <- which(model$inbag[k,] == 0)
predict_res <- predict(model, x[k,], type = "response", predict.all = TRUE)
for (i in 1:50){
no_i <- which(model$inbag[i,index[[k]]] == 0)
is_i <- which(model$inbag[i,index[[k]]] == 1)
upper <- sum(predict_res$individual[,no_i] == y[k])/length(no_i)		# Ratio lower  for classification accuracy and higher for regression MSE.
lower <- sum(predict_res$individual[,is_i] == y[k])/length(is_i)		# Ratio higher for classification accuracy and lower  for regression MSE.
result[k,i] <- upper/lower												# GOOD: ratio < 1 for classification and ratio > 1 for regression.
}
}
diag(result) <- 0
result[1:10,1:10]
diag(result) <- Inf
min(result)
?wtd.var
install.packages('weights')
library(weights)
?wtd.var
rm(list=ls(all=TRUE))
library(randomForest)
library(pheatmap)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Feature', 1:n, sep = '')))
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = FALSE, cluster_cols = FALSE, cex = 1)
model  <- randomForest(x, y,ntree = 1000, replace = TRUE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
result <- matrix(0, m, m)
index  <- list()
for (k in 1:50){
print(k)
index[[k]]  <- which(model$inbag[k,] == 0)
predict_res <- predict(model, x[k,], type = "response", predict.all = TRUE)
for (i in 1:50){
no_i <- which(model$inbag[i,index[[k]]] == 0)
is_i <- which(model$inbag[i,index[[k]]] == 1)
upper <- sum(predict_res$individual[,no_i] == y[k])/length(no_i)		# Ratio lower  for classification accuracy and higher for regression MSE.
lower <- sum(predict_res$individual[,is_i] == y[k])/length(is_i)		# Ratio higher for classification accuracy and lower  for regression MSE.
result[k,i] <- upper/lower												# GOOD: ratio < 1 for classification and ratio > 1 for regression.
}
}
diag(result) <- 0
hic_result <- matrix(0, m, 6, dimnames = list(rownames(x), c("Euc2","Rat2","Euc3","Rat3","Euc4","Rat4")))
distant <- dist(x, method = "euclidean")
dim(distant)
distant
hic_result <- matrix(0, m, 6, dimnames = list(rownames(x), c("Euc2","Rat2","Euc3","Rat3","Euc4","Rat4")))
distant <- dist(x, method = "euclidean")
i
k
k=1
model$inbag[1:10,1:5]
?randomForest
rm(list=ls(all=TRUE))
library(randomForest)
library(pheatmap)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Feature', 1:n, sep = '')))
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = FALSE, cluster_cols = FALSE, cex = 1)
model  <- randomForest(x, y,ntree = 1000, replace = FALSE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
max(model$inbag)
model$inbag[1:10,1:5]
a <- apply(model$inbag,2,sum)
a[1:20]
32/50
?randomForest
ceiling(.632*nrow(x))
rm(list=ls(all=TRUE))
library(randomForest)
library(pheatmap)
m <- 50 											# Sample number
n <- 5                                            # Feature number
x <- matrix(runif(m*n), m, n, dimnames = list(paste('Sample', 1:m, sep = ''), paste('Feature', 1:n, sep = '')))
x[1:(m/2),1:2] <- x[1:(m/2),1:2] + 0.2
y <- gl(2,m/2)										# equivalent to as.factor(c(rep(0,...),rep(1,...)))
pheatmap(x, scale = 'none', border_color = NA, cluster_rows = FALSE, cluster_cols = FALSE, cex = 1)
model  <- randomForest(x, y,ntree = 1000, replace = FALSE, localImp = TRUE, proximity = TRUE, keep.inbag = TRUE)
result <- matrix(0, m, m)
index  <- list()
for (k in 1:50){
print(k)
index[[k]]  <- which(model$inbag[k,] == 0)
predict_res <- predict(model, x[k,], type = "response", predict.all = TRUE)
for (i in 1:50){
no_i <- which(model$inbag[i,index[[k]]] == 0)
is_i <- which(model$inbag[i,index[[k]]] == 1)
upper <- sum(predict_res$individual[,no_i] == y[k])/length(no_i)		# Ratio lower  for classification accuracy and higher for regression MSE.
lower <- sum(predict_res$individual[,is_i] == y[k])/length(is_i)		# Ratio higher for classification accuracy and lower  for regression MSE.
result[k,i] <- upper/lower												# GOOD: ratio < 1 for classification and ratio > 1 for regression.
}
}
diag(result) <- 0
max(result)
diag(result) <- Inf
min(result)
diag(result) <- 0
hic_result <- matrix(0, m, 6, dimnames = list(rownames(x), c("Euc2","Rat2","Euc3","Rat3","Euc4","Rat4")))
distant <- dist(x, method = "euclidean")
hic_info <- hclust(distant, method = "complete")
hic_result[,2*kcent-3] <- cutree(hic_info, kcent)
hic_result <- matrix(0, m, 6, dimnames = list(rownames(x), c("Euc2","Rat2","Euc3","Rat3","Euc4","Rat4")))
distant <- dist(x, method = "euclidean")
for (kcent in 2:4){												# Hierarchical clustering.
hic_info <- hclust(distant, method = "complete")
hic_result[,2*kcent-3] <- cutree(hic_info, kcent)
hic_info <- hclust(as.dist(result), method = "complete")
hic_result[,2*kcent-2] <- cutree(hic_info, kcent)
}
hic_result
dim(hic_result)
hic_result[1:10,]
30.38/2.85
10.66*2.75
30.38*0.95
13500*6.6
16.47+14.14+5.08+54.84
install.packages('Rcpp')
int a = 1;
#include <iostream>
a = 1
a
4019.05-290.58
1200/31*18
10/31*18
20/31
20/31*3
10/31*13
10/31*10+20/31*3
1200/31*18
1200/31*10
1200/31*3
1000-135-50-140.31
1000-135-50-140.31-39.15
180.57/3
a <- 180.57/3
b <- 38.71
a
b
(a-b) *3
741.07-39.15
1st_read_strand <- 1
First_read_strand = 1
dirs <- '/home/exacloud/lustre1/XiaLab/sund/work/201803_Georgiana_Purdy/output'
dirs <- '/home/exacloud/lustre1/XiaLab/sund/work/201803_Georgiana_Purdy/output'
paste(dirs, '/Purdy_Unstranded.RData', sep='')
63.96+11.7+36.02
43.61+49.28
23.31+69.5+25+44.95
30+8.79+19.21+12.5+57.16+31.78+16.98+44.94+7.16
580*6.2
580*6.3
14.95*2+3
59.99+34.6+45.48+95.45
2000-1,761.98
2000-1761.98
library(RegSeq)
Reg_Finding
55000/12
a <- runif(9)
b <- c(1,1,1,2,2,2,3,3,3)
cor(a,b)
b <- c(1,1,1,2,2,2,4,4,4)
cor(a,b)
b <- c(-1,-1,-1,2,2,2,4,4,4)
cor(a,b)
b <- c(-1,-1,-1,2,2,2,40,40,40)
cor(a,b)
b <- c(40,40,40,2,2,2,-1,-1,-1)
cor(a,b)
library(Corbi)
markrank
1.75+0.37+0.4+0.15
library(devtools)
install_github('sunduanchen/RegSeq')
library(RegSeq)
getwd()
setwd('Desktop/RegSeq/packages/RegSeq/vignettes/')
library("RegSeq")
data(PREPROCESSED_ESCA)
?RegSeq
?Reg_Finding
?get_signature
load('ESCA_expression.RData')
header(exprs)
head(exprs)
colnames(exprs)
get_signature
exprs[1,]
exprs[1,,drop=F]
exprs[1,1:8,drop=F]
exprs[1,9:16,drop=F]
sum(exprs[1,1:8,drop=F])
sum(exprs[1,9:16,drop=F])
sum(exprs[2,1:8,drop=F])
sum(exprs[2,9:16,drop=F])
sum(exprs[3,1:8,drop=F])
sum(exprs[3,9:16,drop=F])
sum(exprs[4,1:8,drop=F])
sum(exprs[4,9:16,drop=F])
?Reg_Finding
load('ESCA_regulon.RData')
ls()
class(regulon)
regulon
result <- Reg_Finding(signature, regulon, Permutation = 10, sort = TRUE)
getwd()
library("RegSeq")
library("RegSeq")
options(scipen = 0)
load('ESCA_expression.RData')
dim(exprs)
colnames(exprs)
p <- ncol(exprs)/2
rawCounts <- exprs
groupA <- 1:p
groupB <- (p+1):(2*p)
signature <- get_signature(rawCounts, groupA, groupB)
load('ESCA_regulon.RData')
length(regulon)
result <- Reg_Finding(signature, regulon, Permutation = 10, sort = TRUE)
head(result)
head(result)
class(result)
